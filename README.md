# Expert-Finetune-One-Click

This repository contains a selfâ€‘contained Streamlit application that
guides a subjectâ€‘matter expert through a short questionâ€‘andâ€‘answer
session, builds a training dataset from the responses and
automatically fineâ€‘tunes a LoRA adapter on top of the
`unsloth/Metaâ€‘Llamaâ€‘3.1â€‘8Bâ€‘Instruct` model. The resulting adapter can
then be used to provide domainâ€‘specific responses in Georgian.

## File Structure

```
expert-tune/
â”œâ”€â”€ app.py               # Streamlit frontâ€‘end and Q/A wizard
â”œâ”€â”€ finetune.py          # LoRA training script (GPU)
â”œâ”€â”€ Dockerfile           # For containerising the app
â”œâ”€â”€ requirements.txt     # Python dependencies
â””â”€â”€ README.md            # This file
```

## Usage

1. **Environment Setup**: Install the required dependencies. The
   easiest way is to run inside the provided Docker container, but
   locally you can create a virtual environment and install
   dependencies from `requirements.txt`.

2. **API Key**: Create a `.env` file in the project root with your
   OpenAI API key:

   ```env
   OPENAI_API_KEY=your_openai_api_key_here
   ```

3. **Run the App**: Start the Streamlit application:

   ```bash
   streamlit run app.py
   ```

4. **Answer Questions**: The app will ask you seven concise questions
   about your domain. Provide detailed answers in Georgian.

5. **Fineâ€‘tune**: After answering, click the **ğŸš€ áƒ“áƒáƒ˜áƒ¬áƒ§áƒ” áƒ¤áƒáƒ˜áƒœâ€‘áƒ¢áƒ£áƒœáƒ˜áƒœáƒ’áƒ˜**
   button. The Q/A pairs will be saved to `dataset.jsonl` and the
   LoRA fineâ€‘tuning process will start automatically via
   `finetune.py`. When training completes, the adapter is saved to
   `lora_model`.

## Deployment

There are several ways to deploy this application:

* **Hugging Face Spaces** â€“ Fork this repository on GitHub, set the
  `OPENAI_API_KEY` as a secret in the Spaceâ€™s Variables, select a
  suitable GPU (e.g. A10G) and push. The app will run on
  `https://huggingface.co/spaces/<your-space>`.

* **Render** â€“ Connect your GitHub repository to Render and create a
  new Web Service. Specify the build command (`pip install -r
  requirements.txt`) and the start command (`streamlit run app.py
  --server.port=$PORT --server.address=0.0.0.0`). Ensure you add
  `OPENAI_API_KEY` as an environment variable under **Environment**.

* **Modal** â€“ If you have a `deploy.py` script configured, you can run
  the application via `modal run deploy.py`.

* **Google Colab** â€“ To try it on Google Colab, download `app.py` and
  run it via `streamlit run app.py & npx localtunnel --port 8501` to
  expose it externally.

This repository and its contents are provided under their respective
licenses. Ensure you comply with the terms of the underlying models and
libraries when deploying a derivative model.